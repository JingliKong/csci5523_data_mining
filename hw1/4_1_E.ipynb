{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark \n",
    "import argparse \n",
    "import json \n",
    "\n",
    "# 4.1.1 E: Top n frequent words in the review text\n",
    "\n",
    "sc_conf = pyspark.SparkConf()\n",
    "sc_conf.setAppName('task1')  \n",
    "sc_conf.setMaster('local[*]') \n",
    "sc_conf.set('spark.driver.memory', '8g')\n",
    "sc_conf.set('spark.executor.memory', '4g')\n",
    "# sc = pyspark.SparkContext(conf=sc_conf)\n",
    "sc = pyspark.SparkContext.getOrCreate()\n",
    "sc.setLogLevel(\"OFF\")\n",
    "\n",
    "# read in the lines and create the rdd \n",
    "lines = sc.textFile(\"./data/review.json\")\n",
    "\n",
    "# loading the json into a python object \n",
    "rdd = lines.map(lambda line: json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuations = [\"(\", \"[\", \",\", \".\", \"!\", \"?\", \":\", \";\", \"]\", \")\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now', '']]\n"
     ]
    }
   ],
   "source": [
    "stopwords = []\n",
    "with open('./data/stopwords') as f: \n",
    "    stopwords.append(f.read().split('\\n'))\n",
    "print(stopwords[0:10]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = rdd.map(lambda x: x['text'].replace('!', '').replace('(', '').replace(\"[\",\"\").replace(',','').replace('.', '').replace('?', '').replace(':','').replace(';',''))\n",
    "# first we need to filter by character \n",
    "text = text.map(lambda x: x.lower().split(\" \"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['walked',\n",
       "  'in',\n",
       "  'around',\n",
       "  '4',\n",
       "  'on',\n",
       "  'a',\n",
       "  'friday',\n",
       "  'afternoon',\n",
       "  'we',\n",
       "  'sat',\n",
       "  'at',\n",
       "  'a',\n",
       "  'table',\n",
       "  'just',\n",
       "  'off',\n",
       "  'the',\n",
       "  'bar',\n",
       "  'and',\n",
       "  'walked',\n",
       "  'out',\n",
       "  'after',\n",
       "  '5',\n",
       "  'min',\n",
       "  'or',\n",
       "  'so',\n",
       "  \"don't\",\n",
       "  'even',\n",
       "  'think',\n",
       "  'they',\n",
       "  'realized',\n",
       "  'we',\n",
       "  'walked',\n",
       "  'in',\n",
       "  'however',\n",
       "  'everyone',\n",
       "  'at',\n",
       "  'the',\n",
       "  'bar',\n",
       "  'noticed',\n",
       "  'we',\n",
       "  'walked',\n",
       "  'in',\n",
       "  'service',\n",
       "  'was',\n",
       "  'non',\n",
       "  'existent',\n",
       "  'at',\n",
       "  'best',\n",
       "  'not',\n",
       "  'a',\n",
       "  'good',\n",
       "  'way',\n",
       "  'for',\n",
       "  'a',\n",
       "  'new',\n",
       "  'business',\n",
       "  'to',\n",
       "  'start',\n",
       "  'out',\n",
       "  'oh',\n",
       "  'well',\n",
       "  'the',\n",
       "  'location',\n",
       "  'they',\n",
       "  'are',\n",
       "  'at',\n",
       "  'has',\n",
       "  'been',\n",
       "  'about',\n",
       "  '5',\n",
       "  'different',\n",
       "  'things',\n",
       "  'over',\n",
       "  'the',\n",
       "  'past',\n",
       "  'several',\n",
       "  'years',\n",
       "  'so',\n",
       "  'they',\n",
       "  'will',\n",
       "  'just',\n",
       "  'be',\n",
       "  'added',\n",
       "  'to',\n",
       "  'the',\n",
       "  'list',\n",
       "  'smdh'],\n",
       " ['michael',\n",
       "  'from',\n",
       "  'red',\n",
       "  'carpet',\n",
       "  'vip',\n",
       "  'is',\n",
       "  'amazing',\n",
       "  '',\n",
       "  'i',\n",
       "  'reached',\n",
       "  'out',\n",
       "  'because',\n",
       "  'i',\n",
       "  'needed',\n",
       "  'help',\n",
       "  'planning',\n",
       "  'my',\n",
       "  'soon',\n",
       "  'to',\n",
       "  'be',\n",
       "  'sister',\n",
       "  'in',\n",
       "  \"law's\",\n",
       "  'bachelorette',\n",
       "  'it',\n",
       "  'was',\n",
       "  'a',\n",
       "  'group',\n",
       "  'of',\n",
       "  '10',\n",
       "  'girls',\n",
       "  'so',\n",
       "  'i',\n",
       "  'was',\n",
       "  'a',\n",
       "  'little',\n",
       "  'overwhelmed',\n",
       "  'but',\n",
       "  'michael',\n",
       "  'saved',\n",
       "  'the',\n",
       "  'day',\n",
       "  'everything',\n",
       "  'was',\n",
       "  'super',\n",
       "  'smooth',\n",
       "  'and',\n",
       "  'easy',\n",
       "  'we',\n",
       "  'got',\n",
       "  'good',\n",
       "  'deals',\n",
       "  'and',\n",
       "  'had',\n",
       "  'the',\n",
       "  'best',\n",
       "  'time',\n",
       "  'ever',\n",
       "  'we',\n",
       "  'booked',\n",
       "  'hotel',\n",
       "  'and',\n",
       "  'a',\n",
       "  'bachelorette',\n",
       "  'package',\n",
       "  'for',\n",
       "  'a',\n",
       "  'great',\n",
       "  'price',\n",
       "  'i',\n",
       "  'have',\n",
       "  'saved',\n",
       "  'contact',\n",
       "  'info',\n",
       "  'because',\n",
       "  'i',\n",
       "  'will',\n",
       "  'for',\n",
       "  'sure',\n",
       "  'reach',\n",
       "  'out',\n",
       "  'again',\n",
       "  'on',\n",
       "  'next',\n",
       "  'vegas',\n",
       "  'trip'],\n",
       " ['great',\n",
       "  'lunch',\n",
       "  'today',\n",
       "  'staff',\n",
       "  'was',\n",
       "  'very',\n",
       "  'helpful',\n",
       "  'in',\n",
       "  'assisting',\n",
       "  'with',\n",
       "  'selections',\n",
       "  'and',\n",
       "  'knowledgeable',\n",
       "  'on',\n",
       "  'the',\n",
       "  'ingredients',\n",
       "  'we',\n",
       "  'enjoyed',\n",
       "  'the',\n",
       "  'bbq',\n",
       "  'chicken',\n",
       "  'with',\n",
       "  'tika',\n",
       "  'masala',\n",
       "  'sauce',\n",
       "  'and',\n",
       "  'really',\n",
       "  'good',\n",
       "  'naan',\n",
       "  'bread',\n",
       "  'the',\n",
       "  'biryani',\n",
       "  'with',\n",
       "  'chicken',\n",
       "  'was',\n",
       "  'also',\n",
       "  'yummy',\n",
       "  'fun',\n",
       "  'to',\n",
       "  'see',\n",
       "  'the',\n",
       "  'food',\n",
       "  'being',\n",
       "  'prepared',\n",
       "  'in',\n",
       "  'the',\n",
       "  'tandoori',\n",
       "  'ovens',\n",
       "  'great',\n",
       "  'addition',\n",
       "  'to',\n",
       "  'the',\n",
       "  'fast',\n",
       "  'casual',\n",
       "  'scene',\n",
       "  'in',\n",
       "  'cleveland'],\n",
       " [\"we've\",\n",
       "  'been',\n",
       "  'a',\n",
       "  'huge',\n",
       "  \"slim's\",\n",
       "  'fan',\n",
       "  'since',\n",
       "  'they',\n",
       "  'opened',\n",
       "  'one',\n",
       "  'up',\n",
       "  'in',\n",
       "  'texas',\n",
       "  'about',\n",
       "  'two',\n",
       "  'years',\n",
       "  'ago',\n",
       "  'when',\n",
       "  'we',\n",
       "  'used',\n",
       "  'to',\n",
       "  'live',\n",
       "  'there',\n",
       "  'this',\n",
       "  'place',\n",
       "  'never',\n",
       "  'disappoints',\n",
       "  'they',\n",
       "  'even',\n",
       "  'have',\n",
       "  'great',\n",
       "  'salads',\n",
       "  'and',\n",
       "  'grilled',\n",
       "  'chicken',\n",
       "  'plus',\n",
       "  'they',\n",
       "  'have',\n",
       "  'fresh',\n",
       "  'brewed',\n",
       "  'sweet',\n",
       "  'tea',\n",
       "  \"it's\",\n",
       "  'the',\n",
       "  'best']]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.take(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 10 in stage 130.0 failed 1 times, most recent failure: Lost task 10.0 in stage 130.0 (TID 1286) (192.168.250.233 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/spark/spark-3.3.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n    process()\n  File \"/opt/spark/spark-3.3.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 676, in process\n    out_iter = func(split_index, iterator)\n  File \"/home/lucky/miniconda3/lib/python3.10/site-packages/pyspark/rdd.py\", line 3472, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/home/lucky/miniconda3/lib/python3.10/site-packages/pyspark/rdd.py\", line 3472, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/home/lucky/miniconda3/lib/python3.10/site-packages/pyspark/rdd.py\", line 540, in func\n    return f(iterator)\n  File \"/home/lucky/miniconda3/lib/python3.10/site-packages/pyspark/rdd.py\", line 2554, in combineLocally\n    merger.mergeValues(iterator)\n  File \"/opt/spark/spark-3.3.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/shuffle.py\", line 255, in mergeValues\n    d[k] = comb(d[k], v) if k in d else creator(v)\nTypeError: unhashable type: 'list'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:765)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:747)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2293)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1021)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1020)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat jdk.internal.reflect.GeneratedMethodAccessor123.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/spark/spark-3.3.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n    process()\n  File \"/opt/spark/spark-3.3.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 676, in process\n    out_iter = func(split_index, iterator)\n  File \"/home/lucky/miniconda3/lib/python3.10/site-packages/pyspark/rdd.py\", line 3472, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/home/lucky/miniconda3/lib/python3.10/site-packages/pyspark/rdd.py\", line 3472, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/home/lucky/miniconda3/lib/python3.10/site-packages/pyspark/rdd.py\", line 540, in func\n    return f(iterator)\n  File \"/home/lucky/miniconda3/lib/python3.10/site-packages/pyspark/rdd.py\", line 2554, in combineLocally\n    merger.mergeValues(iterator)\n  File \"/opt/spark/spark-3.3.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/shuffle.py\", line 255, in mergeValues\n    d[k] = comb(d[k], v) if k in d else creator(v)\nTypeError: unhashable type: 'list'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:765)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:747)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [112], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m top_n \u001b[39m=\u001b[39m text\u001b[39m.\u001b[39;49mfilter(\u001b[39mlambda\u001b[39;49;00m x: x \u001b[39mnot\u001b[39;49;00m \u001b[39min\u001b[39;49;00m stopwords) \\\n\u001b[0;32m----> 2\u001b[0m     \u001b[39m.\u001b[39;49mmap(\u001b[39mlambda\u001b[39;49;00m word: (word, \u001b[39m1\u001b[39;49m))\u001b[39m.\u001b[39;49mreduceByKey(\u001b[39mlambda\u001b[39;49;00m a, b: a\u001b[39m+\u001b[39;49mb)\u001b[39m.\u001b[39;49msortBy(\u001b[39mlambda\u001b[39;49;00m x: x[\u001b[39m1\u001b[39;49m], \u001b[39mFalse\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/pyspark/rdd.py:1038\u001b[0m, in \u001b[0;36mRDD.sortBy\u001b[0;34m(self, keyfunc, ascending, numPartitions)\u001b[0m\n\u001b[1;32m   1019\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msortBy\u001b[39m(\n\u001b[1;32m   1020\u001b[0m     \u001b[39mself\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mRDD[T]\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1021\u001b[0m     keyfunc: Callable[[T], \u001b[39m\"\u001b[39m\u001b[39mS\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m   1022\u001b[0m     ascending: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m   1023\u001b[0m     numPartitions: Optional[\u001b[39mint\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1024\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mRDD[T]\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m   1025\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1026\u001b[0m \u001b[39m    Sorts this RDD by the given keyfunc\u001b[39;00m\n\u001b[1;32m   1027\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[39m    [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m   1036\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m   1037\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkeyBy(keyfunc)  \u001b[39m# type: ignore[type-var]\u001b[39;49;00m\n\u001b[0;32m-> 1038\u001b[0m         \u001b[39m.\u001b[39;49msortByKey(ascending, numPartitions)\n\u001b[1;32m   1039\u001b[0m         \u001b[39m.\u001b[39mvalues()\n\u001b[1;32m   1040\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/pyspark/rdd.py:995\u001b[0m, in \u001b[0;36mRDD.sortByKey\u001b[0;34m(self, ascending, numPartitions, keyfunc)\u001b[0m\n\u001b[1;32m    990\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmapPartitions(sortPartition, \u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    992\u001b[0m \u001b[39m# first compute the boundary of each part via sampling: we want to partition\u001b[39;00m\n\u001b[1;32m    993\u001b[0m \u001b[39m# the key-space into bins such that the bins have roughly the same\u001b[39;00m\n\u001b[1;32m    994\u001b[0m \u001b[39m# number of (key, value) pairs falling into them\u001b[39;00m\n\u001b[0;32m--> 995\u001b[0m rddSize \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcount()\n\u001b[1;32m    996\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m rddSize:\n\u001b[1;32m    997\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m  \u001b[39m# empty RDD\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/pyspark/rdd.py:1521\u001b[0m, in \u001b[0;36mRDD.count\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1512\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcount\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mint\u001b[39m:\n\u001b[1;32m   1513\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1514\u001b[0m \u001b[39m    Return the number of elements in this RDD.\u001b[39;00m\n\u001b[1;32m   1515\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1519\u001b[0m \u001b[39m    3\u001b[39;00m\n\u001b[1;32m   1520\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1521\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmapPartitions(\u001b[39mlambda\u001b[39;49;00m i: [\u001b[39msum\u001b[39;49m(\u001b[39m1\u001b[39;49m \u001b[39mfor\u001b[39;49;00m _ \u001b[39min\u001b[39;49;00m i)])\u001b[39m.\u001b[39;49msum()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/pyspark/rdd.py:1508\u001b[0m, in \u001b[0;36mRDD.sum\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1499\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msum\u001b[39m(\u001b[39mself\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mRDD[NumberOrArray]\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mNumberOrArray\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m   1500\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1501\u001b[0m \u001b[39m    Add up the elements in this RDD.\u001b[39;00m\n\u001b[1;32m   1502\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1506\u001b[0m \u001b[39m    6.0\u001b[39;00m\n\u001b[1;32m   1507\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1508\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmapPartitions(\u001b[39mlambda\u001b[39;49;00m x: [\u001b[39msum\u001b[39;49m(x)])\u001b[39m.\u001b[39;49mfold(  \u001b[39m# type: ignore[return-value]\u001b[39;49;00m\n\u001b[1;32m   1509\u001b[0m         \u001b[39m0\u001b[39;49m, operator\u001b[39m.\u001b[39;49madd\n\u001b[1;32m   1510\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/pyspark/rdd.py:1336\u001b[0m, in \u001b[0;36mRDD.fold\u001b[0;34m(self, zeroValue, op)\u001b[0m\n\u001b[1;32m   1331\u001b[0m     \u001b[39myield\u001b[39;00m acc\n\u001b[1;32m   1333\u001b[0m \u001b[39m# collecting result of mapPartitions here ensures that the copy of\u001b[39;00m\n\u001b[1;32m   1334\u001b[0m \u001b[39m# zeroValue provided to each partition is unique from the one provided\u001b[39;00m\n\u001b[1;32m   1335\u001b[0m \u001b[39m# to the final reduce call\u001b[39;00m\n\u001b[0;32m-> 1336\u001b[0m vals \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmapPartitions(func)\u001b[39m.\u001b[39;49mcollect()\n\u001b[1;32m   1337\u001b[0m \u001b[39mreturn\u001b[39;00m reduce(op, vals, zeroValue)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/pyspark/rdd.py:1197\u001b[0m, in \u001b[0;36mRDD.collect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1195\u001b[0m \u001b[39mwith\u001b[39;00m SCCallSiteSync(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontext):\n\u001b[1;32m   1196\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mctx\u001b[39m.\u001b[39m_jvm \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1197\u001b[0m     sock_info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mctx\u001b[39m.\u001b[39;49m_jvm\u001b[39m.\u001b[39;49mPythonRDD\u001b[39m.\u001b[39;49mcollectAndServe(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jrdd\u001b[39m.\u001b[39;49mrdd())\n\u001b[1;32m   1198\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39m(_load_from_socket(sock_info, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jrdd_deserializer))\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1322\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   1324\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 10 in stage 130.0 failed 1 times, most recent failure: Lost task 10.0 in stage 130.0 (TID 1286) (192.168.250.233 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/spark/spark-3.3.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n    process()\n  File \"/opt/spark/spark-3.3.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 676, in process\n    out_iter = func(split_index, iterator)\n  File \"/home/lucky/miniconda3/lib/python3.10/site-packages/pyspark/rdd.py\", line 3472, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/home/lucky/miniconda3/lib/python3.10/site-packages/pyspark/rdd.py\", line 3472, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/home/lucky/miniconda3/lib/python3.10/site-packages/pyspark/rdd.py\", line 540, in func\n    return f(iterator)\n  File \"/home/lucky/miniconda3/lib/python3.10/site-packages/pyspark/rdd.py\", line 2554, in combineLocally\n    merger.mergeValues(iterator)\n  File \"/opt/spark/spark-3.3.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/shuffle.py\", line 255, in mergeValues\n    d[k] = comb(d[k], v) if k in d else creator(v)\nTypeError: unhashable type: 'list'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:765)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:747)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2293)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1021)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1020)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat jdk.internal.reflect.GeneratedMethodAccessor123.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/spark/spark-3.3.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n    process()\n  File \"/opt/spark/spark-3.3.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 676, in process\n    out_iter = func(split_index, iterator)\n  File \"/home/lucky/miniconda3/lib/python3.10/site-packages/pyspark/rdd.py\", line 3472, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/home/lucky/miniconda3/lib/python3.10/site-packages/pyspark/rdd.py\", line 3472, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/home/lucky/miniconda3/lib/python3.10/site-packages/pyspark/rdd.py\", line 540, in func\n    return f(iterator)\n  File \"/home/lucky/miniconda3/lib/python3.10/site-packages/pyspark/rdd.py\", line 2554, in combineLocally\n    merger.mergeValues(iterator)\n  File \"/opt/spark/spark-3.3.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/shuffle.py\", line 255, in mergeValues\n    d[k] = comb(d[k], v) if k in d else creator(v)\nTypeError: unhashable type: 'list'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:765)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:747)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "top_n = text.filter(lambda x: x not in stopwords).map(lambda word: (word, 1)).reduceByKey(lambda a, b: a+b).sortBy(lambda x: x[1], False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('', 6), (\"if i could give this company less than one star i would i am a professional live streamer and online entertainer through twitchtv i have had my personal information leaked on the internet and this company does nothing to protect me or my information i have had two accounts through them one under my own information and one under a friend's information both times a stranger accessed my accounts with nothing but my address or name they give out information over the phone like account holder's name phone number and the last four of your social with only a single identifying measure if someone knows your name address and phone number or 2/3 they can access your account and get your personal information this company will give out and change passwords and pin numbers over the phone they gave out the last four numbers of my ssn do not use this company if you value your identity after your information has been leaked they do nothing to protect you\", 4), (\"i love oregano's and they seem to know how to hire nice and happy people at all locations the servers here are attentive and accommodating my favorite appetizer is the garlic bread with cheese entree is the numero uno stuffed pizza and all of the pizza cookie desserts are awesome\", 3), (\"oregano's has huge salads pasta and great pizzas  they have amazing customer service on par with any high end restaurant  i can recommend any of their salads pasta or their italian gold wings are super moist and tasty one of my favorite pizzas in the valley is their clark street meat pizza  it is a little pricey but really good\", 3), (\"sprint as a company is trash they guarantee that the service has improved and they have the same coverage as verizonall lies i switched from verizon to sprint and that was the biggest mistake the sales rep told me that i could use voice and data at the same time browse the net while on the phone) and that was a lie my signal contantly dropped even while i was at home when i cancelled and ported out they took money out of my account without my consent for the early termination fee sallie mae doesnt even do that the sales rep never explained that to me and all they say in their terrible customer service is it was in your contract do not pay with a debit or credit card you are giving them permission to take money out of your account with or without your consent a billion dollar company gets richer off of snatching money from people's accounts if i could give you no stars i would they hire people to pose as customer service and deal with their sheisty business practices you all ought to be ashamed but im sure that you are not avoid this company at all costs\", 3)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(top_n.take(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(\"received a  mailer and thought okay let's go. walked in on a monday afternoon greeted by a bartender wearing a cute  t shirt that said the bar and below that the baby. how cute however never got her name. looked over menu asked what was good she said the club. so ordered it and  yes it is good. had a beer a club on white toast served with fries and watched sport center. really felt at ease there and good music playing. will go back again.\",\n",
       "  1)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['walked',\n",
       "  'in',\n",
       "  'around',\n",
       "  '4',\n",
       "  'on',\n",
       "  'a',\n",
       "  'friday',\n",
       "  'afternoon,',\n",
       "  'we',\n",
       "  'sat',\n",
       "  'at',\n",
       "  'a',\n",
       "  'table',\n",
       "  'just',\n",
       "  'off',\n",
       "  'the',\n",
       "  'bar',\n",
       "  'and',\n",
       "  'walked',\n",
       "  'out',\n",
       "  'after',\n",
       "  '5',\n",
       "  'min',\n",
       "  'or',\n",
       "  'so.',\n",
       "  \"don't\",\n",
       "  'even',\n",
       "  'think',\n",
       "  'they',\n",
       "  'realized',\n",
       "  'we',\n",
       "  'walked',\n",
       "  'in.',\n",
       "  'however',\n",
       "  'everyone',\n",
       "  'at',\n",
       "  'the',\n",
       "  'bar',\n",
       "  'noticed',\n",
       "  'we',\n",
       "  'walked',\n",
       "  'in!!!',\n",
       "  'service',\n",
       "  'was',\n",
       "  'non',\n",
       "  'existent',\n",
       "  'at',\n",
       "  'best.',\n",
       "  'not',\n",
       "  'a',\n",
       "  'good',\n",
       "  'way',\n",
       "  'for',\n",
       "  'a',\n",
       "  'new',\n",
       "  'business',\n",
       "  'to',\n",
       "  'start',\n",
       "  'out.',\n",
       "  'oh',\n",
       "  'well,',\n",
       "  'the',\n",
       "  'location',\n",
       "  'they',\n",
       "  'are',\n",
       "  'at',\n",
       "  'has',\n",
       "  'been',\n",
       "  'about',\n",
       "  '5',\n",
       "  'different',\n",
       "  'things',\n",
       "  'over',\n",
       "  'the',\n",
       "  'past',\n",
       "  'several',\n",
       "  'years,',\n",
       "  'so',\n",
       "  'they',\n",
       "  'will',\n",
       "  'just',\n",
       "  'be',\n",
       "  'added',\n",
       "  'to',\n",
       "  'the',\n",
       "  'list.',\n",
       "  'smdh!!!']]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = splitting_words.flatMap(lambda line: line.split(' ')) \\\n",
    "    .filter(lambda x: x not in stopwords) \\\n",
    "    .map(lambda word: (word, 1)).reduceByKey(lambda a, b: a+b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = splitting_words.flatMap(lambda line: line.split(' ')) \\\n",
    "    .filter(lambda x: x not in stopwords) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['walked', 'in', 'around']\n"
     ]
    }
   ],
   "source": [
    "print(counts.take(3))\n",
    "# counts = counts.filter(lambda x: x not in stopwords) \n",
    "# print(counts.collect()[0:3])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counts = counts.map(lambda word: (word, 1)).reduceByKey(lambda a, b: a+b)\n",
    "# print(counts.collect()[0:3])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('even', 215856), ('they', 994327), ('noticed', 18323)]\n"
     ]
    }
   ],
   "source": [
    "final = counts.sortBy(lambda x: x[1], False)\n",
    "print(counts.collect()[0:3])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(final.collect()[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "49c7475afa802cdf659320c820863a127978f36b368c817ac9cb102b34b4cef7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
